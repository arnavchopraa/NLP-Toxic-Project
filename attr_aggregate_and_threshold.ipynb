{"cells":[{"cell_type":"markdown","metadata":{},"source":["Run this notebook after attribute.ipynb to aggregate the tokens into full words and filter out only words with an attribution score above a certain threshold.\n"]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"}},"source":["!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","!pip install captum\n","\n","from captum.attr import FeatureAblation, LLMAttribution, TextTokenInput\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n","from huggingface_hub import login\n","import torch\n","import json\n","\n","login(\"<HUGGINGFACE_API_KEY>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:20:06.550894Z","iopub.status.busy":"2024-06-05T09:20:06.550255Z","iopub.status.idle":"2024-06-05T09:22:18.999430Z","shell.execute_reply":"2024-06-05T09:22:18.998442Z","shell.execute_reply.started":"2024-06-05T09:20:06.550861Z"},"trusted":true},"outputs":[],"source":["quantization_config = BitsAndBytesConfig(load_in_8bit=False,load_in_4bit=True)\n","\n","model = 'llama'     # 'bloom' / 'llama' / 'mistral'\n","\n","model_id = {\n","    'bloom': \"bigscience/bloom-7b1\",\n","    'llama': \"meta-llama/Meta-Llama-3-8B\",\n","    'mistral': \"mistralai/Mistral-7B-v0.1\"\n","}[model]\n","\n","model_4bit = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map = \"auto\",\n","    quantization_config=quantization_config\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:22:19.001740Z","iopub.status.busy":"2024-06-05T09:22:19.001447Z","iopub.status.idle":"2024-06-05T09:22:19.005881Z","shell.execute_reply":"2024-06-05T09:22:19.004978Z","shell.execute_reply.started":"2024-06-05T09:22:19.001714Z"},"trusted":true},"outputs":[],"source":["fa = FeatureAblation(model_4bit)\n","llm = LLMAttribution(fa, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T10:57:05.318547Z","iopub.status.busy":"2024-06-05T10:57:05.317388Z","iopub.status.idle":"2024-06-05T10:57:05.338214Z","shell.execute_reply":"2024-06-05T10:57:05.337065Z","shell.execute_reply.started":"2024-06-05T10:57:05.318499Z"},"trusted":true},"outputs":[],"source":["# Load attributions\n","file = f\"attributions/{model}_attr_output.pt\"\n","\n","data = torch.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T10:57:12.716380Z","iopub.status.busy":"2024-06-05T10:57:12.715274Z","iopub.status.idle":"2024-06-05T10:57:19.562723Z","shell.execute_reply":"2024-06-05T10:57:19.561699Z","shell.execute_reply.started":"2024-06-05T10:57:12.716326Z"},"trusted":true},"outputs":[],"source":["attributions = []\n","\n","c = 1\n","for d in data:\n","    # Make dummy attribution object and overwrite with loaded attribution\n","    attr = llm.attribute(TextTokenInput(\"ab cg\", tokenizer), \"bc cg\")\n","    attr.__dict__ = d\n","    attributions.append(attr)\n","    \n","    # Counter\n","    print(f\"{c}/{len(data)}\")\n","    c += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T11:54:08.095466Z","iopub.status.busy":"2024-06-05T11:54:08.095051Z","iopub.status.idle":"2024-06-05T11:54:20.351252Z","shell.execute_reply":"2024-06-05T11:54:20.349971Z","shell.execute_reply.started":"2024-06-05T11:54:08.095436Z"},"trusted":true},"outputs":[],"source":["# Method for aggregating subwords into full words. \n","def aggregate_attr_cols(attribution_dict):\n","    # Get rid of first word, which is \"<|begin_of_text|>\"\n","    inp = attribution_dict['input_tokens'][1:]\n","    out = attribution_dict['output_tokens']\n","    token = attribution_dict['token_attr'][:, 1:]\n","    seq = attribution_dict['seq_attr'][1:]\n","    \n","    # Indexes of the start of every word. Every new word starts with 'Ġ', 'Ċ' or '▁'\n","    merge_cols = [0] + [i for i, v in enumerate(inp) if (('Ġ' in v) or ('Ċ' in v)) or ('▁' in v)]   \n","\n","    new_inps = []\n","    tcols = []\n","    scols = []\n","    for i, c in enumerate(merge_cols):\n","        # Last word goes until the end of the list, all others go until the next 'Ġ' i.e. start of the next word\n","        if i == len(merge_cols)-1:\n","            to = len(inp)\n","        else:\n","            to = merge_cols[i+1]\n","\n","        # Initialise the new input word, and the token and seq attributions for the word\n","        new_inp = inp[c]\n","        tcol = token[:, c].detach().clone()\n","        scol = seq[c].detach().clone()\n","\n","        # For all subwords until the start of the next word, add the subword and attributions.\n","        for j in range(c+1, to):\n","            new_inp += inp[j]\n","            tcol += token[:, j]\n","            scol += seq[j]\n","\n","        # Remove special characters\n","        remove_chars = 'âĢľĠĻĿĊ▁'\n","        new_inp = ''.join([c for c in new_inp if not c in remove_chars])\n","\n","        new_inps.append(new_inp)\n","        tcols.append(tcol)\n","        scols.append(scol)\n","\n","    new_dict = {}\n","    new_dict['input_tokens'] = new_inps\n","    new_dict['output_tokens'] = out\n","    new_dict['token_attr'] = torch.transpose(torch.stack(tcols), 0, 1)\n","    new_dict['seq_attr'] = torch.stack(scols)\n","    \n","    return new_dict\n","    \n","def aggregate_attr_rows(attribution_dict):\n","    inp = attribution_dict['input_tokens']\n","    out = attribution_dict['output_tokens']\n","    token = attribution_dict['token_attr']\n","    seq = attribution_dict['seq_attr']\n","    \n","    # Indexes of the start of every word. Every new word starts with 'Ġ', 'Ċ' or '▁'\n","    merge_rows = [i for i, v in enumerate(out) if (('Ġ' in v) or ('Ċ' in v)) or ('▁' in v)]  \n","\n","    new_outs = []\n","    trows = []\n","    for i, c in enumerate(merge_rows):\n","        # Last word goes until the end of the list, all others go until the next 'Ġ' i.e. start of the next word\n","        if i == len(merge_rows)-1:\n","            to = len(out)\n","        else:\n","            to = merge_rows[i+1]\n","            \n","        # Initialise the new input word, and the token attributions for the word\n","        new_out = out[c]\n","        trow = token[c].detach().clone()\n","        \n","        # For all subwords until the start of the next word, add the subword and attributions.\n","        for j in range(c+1, to):\n","            new_out += out[j]\n","            trow += token[j]\n","            \n","        # Remove special characters\n","        remove_chars = 'âĢľĠĻĿĊ▁'\n","        new_out = ''.join([c for c in new_out if not c in remove_chars])\n","        new_outs.append(new_out)\n","        trows.append(trow)\n","    \n","    new_dict = {}\n","    new_dict['input_tokens'] = inp\n","    new_dict['output_tokens'] = new_outs\n","    new_dict['token_attr'] = torch.stack(trows)\n","    new_dict['seq_attr'] = seq\n","    \n","    return new_dict\n","\n","# Make dummy attribution object and overwrite with aggregated attribution\n","# We do it this way with dummies to copy the data so we dont lose the original unaggregated data\n","attr = llm.attribute(TextTokenInput(\"ab cg\", tokenizer), \"bc cg\")\n","attr.__dict__ = aggregate_attr_rows(aggregate_attr_cols(attributions[0].__dict__))\n","attributions[0].plot_token_attr(show=True)\n","attr.plot_token_attr(show=True)\n","\n","attributions[0].plot_seq_attr(show=True)\n","attr.plot_seq_attr(show=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the prompts and generated continuation\n","sentences = []\n","for x in open(f\"results/{model}/most_toxic.jsonl\",\"r\").readlines():\n","    j = json.loads(x)\n","    sentences.append(j[\"prompt\"] + j[\"generated\"])\n","\n","# Aggregate attributions\n","aggregated = [aggregate_attr_rows(aggregate_attr_cols(i.__dict__)) for i in attributions]\n","\n","attributed_to = []\n","\n","for d in aggregated:\n","    att = {}\n","\n","    # Iterate over all attributions and store the indices where the attribution is >= 2\n","    for i, row in enumerate(d[\"token_attr\"].detach()):\n","        for j, elem in enumerate(row):\n","            # For each input token store the output tokens that have an attribution >= 2\n","            inp = d[\"input_tokens\"][j]\n","            if not inp in att:\n","                att[inp] = []\n","\n","            if elem >= 2:\n","                att[inp].append(d[\"output_tokens\"][i])\n","    \n","    attributed_to.append(att)\n","\n","# Write to file\n","write_dir = f\"aggregated_attr/{model}_attr_threshold.txt\"\n","\n","with open(write_dir,\"w\") as f:\n","    for i, a in enumerate(attributed_to):\n","        f.write(sentences[i] + \"\\n\" + json.dumps(a) + \"\\n\\n\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5101972,"sourceId":8611454,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
