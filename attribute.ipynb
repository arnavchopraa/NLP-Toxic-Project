{"cells":[{"cell_type":"markdown","metadata":{},"source":["Run this notebook after get_most_toxic.py to get the attribution scores for all the generated output tokens (continuations) wrt the input tokens (prompts).\n","\n","After this notebook run attr_aggregate_and_threshold.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","!pip install captum\n","\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n","from huggingface_hub import login\n","from captum.attr import FeatureAblation, LLMAttribution, TextTokenInput\n","import json\n","import torch\n","\n","login(\"<HUGGINGFACE_API_TOKEN>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["quantization_config = BitsAndBytesConfig(load_in_8bit=False,load_in_4bit=True)\n","\n","model = 'mistral'     # 'bloom' / 'llama' / 'mistral'\n","\n","model_id = {\n","    'bloom': \"bigscience/bloom-7b1\",\n","    'llama': \"meta-llama/Meta-Llama-3-8B\",\n","    'mistral': \"mistralai/Mistral-7B-v0.1\"\n","}[model]\n","\n","model_4bit = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map = \"auto\",\n","    quantization_config=quantization_config\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fa = FeatureAblation(model_4bit)\n","llm = LLMAttribution(fa, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["file = f\"results/{model}/most_toxic.jsonl\"\n","outs = []\n","\n","for line in open(file).readlines():\n","    message = json.loads(line)\n","    i = message[\"prompt\"]\n","    o = message[\"generated\"]\n","    inp = TextTokenInput(i, tokenizer)\n","\n","    # Get attributions\n","    out = llm.attribute(inp, target=o, show_progress=True)\n","    outs.append(out.__dict__)\n","\n","# Save attributions\n","torch.save(outs, f'attributions/{model}_attr_output.pt')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5101972,"sourceId":8597619,"sourceType":"datasetVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
